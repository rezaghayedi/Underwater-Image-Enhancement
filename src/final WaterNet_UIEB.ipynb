{"cells":[{"cell_type":"markdown","source":["# Introduction"],"metadata":{"id":"4x9VW1cDCiBz"}},{"cell_type":"markdown","source":["Underwater image enhancement is a challenging task in the field of computer vision due to issues such as light absorption, scattering, and color distortion caused by the water medium. These distortions significantly degrade image quality and affect the performance of downstream tasks like object detection, classification, and navigation in underwater environments. In this project, we address these challenges by employing a deep learning-based enhancement technique using the WaterNet model. WaterNet is specifically designed to restore underwater images by correcting color shifts, enhancing contrast, and recovering structural details. We trained and evaluated the model using the UIEB dataset, which contains a diverse set of real-world underwater images and corresponding ground truths. The goal is to improve visual clarity and make underwater images more suitable for further computer vision applications."],"metadata":{"id":"nTI7s5i4CDB7"}},{"cell_type":"markdown","source":["# Step 1: How to access dataset\n"],"metadata":{"id":"0rmcGXkpfP64"}},{"cell_type":"markdown","source":["The dataset is publicly available on Google Drive at the following address. It can be downloaded using the command below:"],"metadata":{"id":"s7tLf9b_fU9J"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39119,"status":"ok","timestamp":1763300272877,"user":{"displayName":"Reza Ghaedi","userId":"06294103763002776804"},"user_tz":0},"id":"fT3x_cxdExaF","outputId":"ec2ba309-0647-4f7d-8351-f216d506f502"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","From (original): https://drive.google.com/uc?id=1adx6d7BNMc7KqHA5hLaInRkiZiYDGoZO\n","From (redirected): https://drive.google.com/uc?id=1adx6d7BNMc7KqHA5hLaInRkiZiYDGoZO&confirm=t&uuid=6eb6c5eb-7b30-4445-9a70-a468264d55d6\n","To: /content/UIEB_RAW_REF_splitted-zip.zip\n","100% 1.49G/1.49G [00:24<00:00, 59.8MB/s]\n"]}],"source":["!gdown --fuzzy \"https://drive.google.com/uc?id=1adx6d7BNMc7KqHA5hLaInRkiZiYDGoZO\"\n","!unzip -q UIEB_RAW_REF_splitted-zip.zip -d UIEB_Dataset"]},{"cell_type":"markdown","source":["# Step 2: Import Necessary Libraries"],"metadata":{"id":"6AG5q9rrfb9V"}},{"cell_type":"markdown","source":["All essential libraries and modules required for constructing, training, and evaluating the deep learning model using PyTorch are imported."],"metadata":{"id":"cEzz5KthfnAH"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"mGBiUTHlF6YV","executionInfo":{"status":"ok","timestamp":1763300295479,"user_tz":0,"elapsed":10388,"user":{"displayName":"Reza Ghaedi","userId":"06294103763002776804"}}},"outputs":[],"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import numpy as np\n","from torchvision.datasets import ImageFolder\n","from torchvision.utils import save_image"]},{"cell_type":"markdown","source":["# Step 3: Dataset Preparation"],"metadata":{"id":"Go1cMyWZfw-S"}},{"cell_type":"markdown","source":["The next section defines a custom dataset class, UIEBDataset, which is designed to load and preprocess paired input-target images from the UIEB dataset. Paths to the input and target image directories, along with an optional transformation function, are provided to the class. Methods for loading images, applying transformations (e.g., resizing, normalization), and returning processed images as tensors are included. The dataset is organized into separate directories for training and validation. DataLoader objects are created for both datasets to facilitate efficient batching, shuffling, and data loading during model training and evaluation."],"metadata":{"id":"VcQBz3V7gaaz"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"Iph4biEAF6bM","executionInfo":{"status":"ok","timestamp":1763300295484,"user_tz":0,"elapsed":2,"user":{"displayName":"Reza Ghaedi","userId":"06294103763002776804"}}},"outputs":[],"source":["class UIEBDataset(Dataset):\n","    def __init__(self, input_dir, target_dir, transform=None):\n","        self.input_dir = input_dir\n","        self.target_dir = target_dir\n","        self.input_images = sorted(os.listdir(input_dir))\n","        self.target_images = sorted(os.listdir(target_dir))\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.input_images)\n","\n","    def __getitem__(self, idx):\n","        input_path = os.path.join(self.input_dir, self.input_images[idx])\n","        target_path = os.path.join(self.target_dir, self.target_images[idx])\n","\n","        input_img = Image.open(input_path).convert('RGB')\n","        target_img = Image.open(target_path).convert('RGB')\n","\n","        if self.transform:\n","            input_img = self.transform(input_img)\n","            target_img = self.transform(target_img)\n","\n","        return input_img, target_img\n"]},{"cell_type":"markdown","source":["The directory paths for the training and validation datasets are established in this cell. The locations of the main folders within the UIEB dataset, stored on Google Drive, are specified for both the training and validation splits. Each dataset split is organized into distinct subdirectories containing the input images (original underwater images) and the target images (enhanced or reference versions). These paths facilitate the subsequent loading and processing of data during model training and validation."],"metadata":{"id":"r5F2rXjMDgi-"}},{"cell_type":"code","source":["train_dir = \"/content/UIEB_Dataset/UIEB_RAW_REF_splitted/train\"\n","train_input_dir = train_dir + \"/\" + \"input\"\n","train_target_dir = train_dir + \"/\" + \"target\"\n","\n","val_dir = \"/content/UIEB_Dataset/UIEB_RAW_REF_splitted/val\"\n","val_input_dir = val_dir + \"/\" + \"input\"\n","val_target_dir = val_dir + \"/\" + \"target\"\n"],"metadata":{"id":"rmA1K67vbPTs","executionInfo":{"status":"ok","timestamp":1763300328814,"user_tz":0,"elapsed":4,"user":{"displayName":"Reza Ghaedi","userId":"06294103763002776804"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["A series of image transformations is applied in this cell to prepare the data for model training. The transformations include resizing images to 256x256 pixels, converting them into tensor format, and normalizing their pixel values to a range of [-1, 1]. These preprocessing steps help standardize the input data and improve model performance. Following this, instances of the custom dataset class UIEBDataset are created for both the training and validation sets, incorporating the specified transformations to ensure consistent data handling during the training and evaluation phases."],"metadata":{"id":"HGsAWTLvDoZP"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"K19yYpZJF6d0","executionInfo":{"status":"ok","timestamp":1763300331752,"user_tz":0,"elapsed":21,"user":{"displayName":"Reza Ghaedi","userId":"06294103763002776804"}}},"outputs":[],"source":["transform = transforms.Compose([\n","      transforms.Resize((256, 256)),\n","      transforms.ToTensor(),\n","      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # for normalize to [-1, 1]\n","    ])\n","\n","train_dataset = UIEBDataset(train_input_dir, train_target_dir, transform=transform)\n","val_dataset = UIEBDataset(val_input_dir, val_target_dir, transform=transform)\n"]},{"cell_type":"markdown","source":["DataLoader objects are instantiated in this cell to facilitate efficient data handling during training and validation. The training DataLoader is configured with a batch size of 8 and shuffling enabled to ensure that the data is randomly sampled in each epoch, promoting better generalization. The validation DataLoader uses the same batch size but disables shuffling to maintain a consistent order for evaluation. These DataLoaders enable streamlined batching and loading of the datasets during the model’s training and validation processes."],"metadata":{"id":"RO8mlAnHDzJZ"}},{"cell_type":"code","execution_count":6,"metadata":{"id":"Ru9dWVHvwzIh","executionInfo":{"status":"ok","timestamp":1763300344881,"user_tz":0,"elapsed":5,"user":{"displayName":"Reza Ghaedi","userId":"06294103763002776804"}}},"outputs":[],"source":["train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n"]},{"cell_type":"markdown","source":["# Step 4: Enhanced WaterNet Model"],"metadata":{"id":"0JXSeDu5D1Kb"}},{"cell_type":"markdown","source":["The following code defines an enhanced deep learning model architecture designed for image restoration tasks, such as underwater image enhancement. The model incorporates advanced components to improve feature representation and learning capacity.\n","\n","Specifically, it includes a Squeeze-and-Excitation (SE) attention block that adaptively recalibrates channel-wise feature responses, enhancing the model’s ability to focus on important image details. Residual blocks are used to facilitate efficient gradient flow and deeper network design by adding shortcut connections, which help the model learn more complex mappings without degradation.\n","\n","The overall architecture, named WaterNetPlus, consists of an encoder to extract features, a middle processing stage to refine them, and a decoder to reconstruct the enhanced image output. The design leverages convolutional layers, nonlinear activations, batch normalization, and attention mechanisms to effectively restore degraded images. This modular and hierarchical structure allows the model to capture both low-level and high-level image features for improved restoration performance.\n","More discussion will be provided:\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"a2eXM4rWENnx"}},{"cell_type":"markdown","source":["SEBlock (Squeeze-and-Excitation Block):\n","This module introduces a channel-wise attention mechanism that adaptively recalibrates feature responses. It achieves this by first aggregating spatial information through global average pooling, then passing the result through a small bottleneck network with nonlinear activations, and finally applying a sigmoid function to generate per-channel weights. These weights are used to emphasize the most informative features in each channel, improving the model’s focus on relevant details.\n","\n","ResidualBlock with SE Attention:\n","This block consists of two convolutional layers, each followed by batch normalization and ReLU activation functions. After these convolutions, the SEBlock is applied to enhance the representational capacity through attention. Additionally, a residual connection adds the block’s input to its output, which helps preserve the original information and facilitates more effective training by mitigating issues such as gradient vanishing.\n","\n","WaterNetPlus Model Architecture:\n","The overall architecture is divided into three main components:\n","\n","Encoder: Extracts hierarchical features from the input image via an initial convolutional layer followed by several ResidualBlocks equipped with SE attention.\n","\n","Middle: Processes the encoded features further with an additional convolution and ResidualBlock to refine the representations.\n","\n","Decoder: Reconstructs the output image from the refined features using a convolutional layer and a Tanh activation function, which normalizes the pixel values to a range between -1 and 1."],"metadata":{"id":"MlOT5jV6EiKp"}},{"cell_type":"code","source":["# Enhanced version(WaterNet+):\n","\n","# SE Attention Block\n","class SEBlock(nn.Module):\n","    def __init__(self, channels, reduction=16):\n","        super(SEBlock, self).__init__()\n","        self.fc = nn.Sequential(\n","            nn.AdaptiveAvgPool2d(1),\n","            nn.Conv2d(channels, channels // reduction, kernel_size=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(channels // reduction, channels, kernel_size=1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        w = self.fc(x)\n","        return x * w\n","\n","# Residual Block with SE Attention\n","class ResidualBlock(nn.Module):\n","    def __init__(self, channels):\n","        super(ResidualBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n","        self.se = SEBlock(channels)  # SE Attention Block\n","\n","        # Batch Normalization\n","        self.bn1 = nn.BatchNorm2d(channels)\n","        self.bn2 = nn.BatchNorm2d(channels)\n","\n","    def forward(self, x):\n","        identity = x\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.se(out)  # SE attention after second conv\n","        out += identity\n","        out = self.relu(out)\n","        return out\n","\n","# WaterNet+ Model with improved architecture\n","class WaterNetPlus(nn.Module):\n","    def __init__(self):\n","        super(WaterNetPlus, self).__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            ResidualBlock(64),\n","            ResidualBlock(64),\n","            ResidualBlock(64)  # Additional Residual Block\n","        )\n","        self.middle = nn.Sequential(\n","            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            ResidualBlock(64)\n","        )\n","        self.decoder = nn.Sequential(\n","            nn.Conv2d(64, 3, kernel_size=3, padding=1),\n","            nn.Tanh()  # [-1, 1] for final output\n","        )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.middle(x)\n","        x = self.decoder(x)\n","        return x\n"],"metadata":{"id":"KUlgBzu4cIVX","executionInfo":{"status":"ok","timestamp":1763300375324,"user_tz":0,"elapsed":29,"user":{"displayName":"Reza Ghaedi","userId":"06294103763002776804"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["This code determines whether a GPU (CUDA) is available for computation and sets the device accordingly. If a compatible GPU is present, it selects \"cuda\" to leverage faster processing; otherwise, it defaults to the CPU. The chosen device is then printed to inform the user which hardware will be used for running the model and computations.\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"Im89GtxxFRqe"}},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"prBf21bcTl9f","executionInfo":{"status":"ok","timestamp":1763300410060,"user_tz":0,"elapsed":9,"user":{"displayName":"Reza Ghaedi","userId":"06294103763002776804"}},"outputId":"b8e10c75-e92e-49ff-96bd-1788f55e6c73"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}]},{"cell_type":"code","execution_count":9,"metadata":{"id":"ALAB4BqQYM5x","executionInfo":{"status":"ok","timestamp":1763300415332,"user_tz":0,"elapsed":177,"user":{"displayName":"Reza Ghaedi","userId":"06294103763002776804"}}},"outputs":[],"source":["model_enhanced = WaterNetPlus().to(device)"]},{"cell_type":"code","source":["model = model_enhanced"],"metadata":{"id":"mPwubH2E0N5F","executionInfo":{"status":"ok","timestamp":1763300418863,"user_tz":0,"elapsed":3,"user":{"displayName":"Reza Ghaedi","userId":"06294103763002776804"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["# Step 5: Image Quality Evaluation Metrics"],"metadata":{"id":"msXzGTEKFXcx"}},{"cell_type":"markdown","source":["The following code focuses on evaluating image restoration quality by computing two widely used metrics: **PSNR** (Peak Signal-to-Noise Ratio) and **SSIM** (Structural Similarity Index Measure). These metrics quantify how closely the restored (predicted) images resemble their original (target) counterparts, serving as objective measures of image fidelity.\n","\n","Conceptual Overview:\n","PSNR measures the ratio between the maximum possible power of a signal (image) and the power of corrupting noise that affects the fidelity of its representation. Higher PSNR values generally indicate better quality, meaning the restored image is closer to the original.\n","\n","SSIM assesses image similarity based on luminance, contrast, and structural information. It is designed to be more perceptually relevant than PSNR by considering how humans perceive changes in image quality. SSIM values range from -1 to 1, where 1 indicates perfect similarity."],"metadata":{"id":"CciITP1zFhIE"}},{"cell_type":"markdown","source":["**Code Explanation:**\n","The two functions, **calculate_psnr** and **calculate_ssim**, take as inputs the predicted images and their corresponding ground truth targets. Both tensors are first converted from PyTorch tensors to NumPy arrays after detaching from the computation graph and moving to the CPU.\n","\n","Because these arrays have channel-first format (C, H, W), they are transposed to the common image format (H, W, C) to be compatible with the metric functions from the skimage library.\n","\n","For each image pair, the respective metric is calculated and accumulated. Finally, the average value over the entire batch is returned as the evaluation score.\n","\n","This approach enables batch-wise assessment of restoration performance during or after training deep learning models for tasks like denoising, enhancement, or super-resolution."],"metadata":{"id":"FOmLUNc9GDM_"}},{"cell_type":"code","execution_count":11,"metadata":{"id":"r4H1KZSnOFW2","executionInfo":{"status":"ok","timestamp":1763300440299,"user_tz":0,"elapsed":548,"user":{"displayName":"Reza Ghaedi","userId":"06294103763002776804"}}},"outputs":[],"source":["from skimage.metrics import peak_signal_noise_ratio as psnr_metric\n","from skimage.metrics import structural_similarity as ssim_metric\n","\n","def calculate_psnr(pred, target):\n","    pred = pred.detach().cpu().numpy()\n","    target = target.detach().cpu().numpy()\n","    psnr_val = 0\n","    for p, t in zip(pred, target):\n","        p = np.transpose(p, (1, 2, 0))  # (H, W, C)\n","        t = np.transpose(t, (1, 2, 0))\n","        psnr_val += psnr_metric(t, p, data_range=2.0)\n","    return psnr_val / pred.shape[0] # avg PNSR\n","\n","def calculate_ssim(pred, target):\n","    pred = pred.detach().cpu().numpy()\n","    target = target.detach().cpu().numpy()\n","    ssim_val = 0\n","    for p, t in zip(pred, target):\n","        p = np.transpose(p, (1, 2, 0))  # (H, W, C)\n","        t = np.transpose(t, (1, 2, 0))\n","        ssim_val += ssim_metric(t, p, channel_axis=2, data_range=2.0)\n","    return ssim_val / pred.shape[0]  # avg SSIM\n"]},{"cell_type":"markdown","source":["# Step 6: Model Training and Validation Procedure"],"metadata":{"id":"nQkG-74RGaEy"}},{"cell_type":"markdown","source":["The following function(**train_model**) manages the complete training and validation process of the deep learning model. It iterates through a specified number of epochs, during which the model learns to map input images to their corresponding target images by minimizing a loss function.\n","\n","In each epoch, the model is set to training mode to enable weight updates. Batches of training data are processed sequentially: the inputs are passed through the model, the loss between predictions and targets is computed, and gradients are backpropagated to optimize the model parameters via the selected optimizer.\n","\n","After training, the model switches to evaluation mode, where it processes validation data without updating weights. During validation, the loss is calculated to monitor how well the model generalizes to unseen data. Additionally, two quantitative image quality metrics **PSNR** and **SSIM** are computed to provide more detailed insight into the fidelity of the reconstructed images.\n","\n","A **learning rate scheduler** is used to progressively reduce the learning rate, facilitating more stable convergence.\n","\n","The function keeps track of the best validation loss achieved and saves the model’s parameters when a new minimum is observed, ensuring that the best-performing model is preserved for later use."],"metadata":{"id":"c2DxVcdpGZTG"}},{"cell_type":"code","execution_count":12,"metadata":{"id":"-bir4UtaOGmq","executionInfo":{"status":"ok","timestamp":1763300457957,"user_tz":0,"elapsed":6,"user":{"displayName":"Reza Ghaedi","userId":"06294103763002776804"}}},"outputs":[],"source":["def train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs, save_dir=\"./\"):\n","    best_val_loss = float('inf')\n","\n","    # Learning Rate Scheduler\n","    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)  # Decrease LR every 10 epoch\n","    for epoch in range(num_epochs):\n","        model.train()\n","        running_loss = 0.0\n","\n","        for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\", leave=False):\n","            inputs = inputs.to(device)\n","            targets = targets.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, targets)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","        train_loss = running_loss / len(train_loader)\n","\n","        model.eval()\n","        val_loss = 0.0\n","        total_psnr = 0.0\n","        total_ssim = 0.0\n","\n","        with torch.no_grad():\n","            for inputs, targets in tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\", leave=False):\n","                inputs = inputs.to(device)\n","                targets = targets.to(device)\n","\n","                outputs = model(inputs)\n","                loss = criterion(outputs, targets)\n","                val_loss += loss.item()\n","\n","                total_psnr += calculate_psnr(outputs, targets)\n","                total_ssim += calculate_ssim(outputs, targets)\n","\n","        val_loss /= len(val_loader)\n","        avg_psnr = total_psnr / len(val_loader)\n","        avg_ssim = total_ssim / len(val_loader)\n","\n","        print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n","              f\"Train Loss: {train_loss:.4f} \"\n","              f\"Val Loss: {val_loss:.4f} \"\n","              f\"PSNR: {avg_psnr:.2f} \"\n","              f\"SSIM: {avg_ssim:.4f}\")\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            model_path = os.path.join(save_dir, \"best_waternet.pth\")\n","            torch.save(model.state_dict(), model_path)\n","            print(f\"Best model saved at epoch {epoch+1} with Val Loss: {val_loss:.4f}\")\n","\n","        scheduler.step()  # Decrease LR"]},{"cell_type":"markdown","source":["The loss function is defined as Mean Squared Error (MSELoss), which measures the average squared difference between the predicted and target outputs. The optimization algorithm selected is Adam, initialized with a learning rate of 0.001 to update the model parameters during training. Additionally, a directory path is specified for saving the trained model checkpoints and related files."],"metadata":{"id":"SaeKWwlrHJRR"}},{"cell_type":"code","execution_count":13,"metadata":{"id":"EQivs1pVON52","executionInfo":{"status":"ok","timestamp":1763300578024,"user_tz":0,"elapsed":19,"user":{"displayName":"Reza Ghaedi","userId":"06294103763002776804"}}},"outputs":[],"source":["criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","\n","save_dir = \"/content/Waternet_Model\"\n"]},{"cell_type":"markdown","source":["The training process is set to run for 50 epochs, enabling the model to iteratively learn from the training data."],"metadata":{"id":"R6jQIaMaHVg6"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ITFGRuQTE__i","outputId":"c1b9a147-d8c9-43c9-c72d-a6f8af39b7b1","executionInfo":{"status":"ok","timestamp":1747141968989,"user_tz":-60,"elapsed":5379821,"user":{"displayName":"Reza Ghaedi","userId":"06294103763002776804"}}},"outputs":[{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [1/50] Train Loss: 0.0865 Val Loss: 0.0642 PSNR: 18.79 SSIM: 0.5193\n","Best model saved at epoch 1 with Val Loss: 0.0642\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [2/50] Train Loss: 0.0565 Val Loss: 0.0548 PSNR: 19.51 SSIM: 0.5748\n","Best model saved at epoch 2 with Val Loss: 0.0548\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [3/50] Train Loss: 0.0494 Val Loss: 0.0494 PSNR: 20.10 SSIM: 0.6012\n","Best model saved at epoch 3 with Val Loss: 0.0494\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [4/50] Train Loss: 0.0443 Val Loss: 0.0463 PSNR: 20.30 SSIM: 0.6044\n","Best model saved at epoch 4 with Val Loss: 0.0463\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [5/50] Train Loss: 0.0422 Val Loss: 0.0398 PSNR: 20.99 SSIM: 0.6437\n","Best model saved at epoch 5 with Val Loss: 0.0398\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [6/50] Train Loss: 0.0403 Val Loss: 0.0426 PSNR: 20.85 SSIM: 0.6357\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [7/50] Train Loss: 0.0394 Val Loss: 0.0384 PSNR: 21.25 SSIM: 0.6583\n","Best model saved at epoch 7 with Val Loss: 0.0384\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [8/50] Train Loss: 0.0373 Val Loss: 0.0358 PSNR: 21.51 SSIM: 0.6599\n","Best model saved at epoch 8 with Val Loss: 0.0358\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [9/50] Train Loss: 0.0355 Val Loss: 0.0386 PSNR: 21.23 SSIM: 0.6494\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [10/50] Train Loss: 0.0349 Val Loss: 0.0333 PSNR: 21.97 SSIM: 0.6809\n","Best model saved at epoch 10 with Val Loss: 0.0333\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [11/50] Train Loss: 0.0305 Val Loss: 0.0318 PSNR: 22.23 SSIM: 0.6893\n","Best model saved at epoch 11 with Val Loss: 0.0318\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [12/50] Train Loss: 0.0296 Val Loss: 0.0312 PSNR: 22.37 SSIM: 0.6924\n","Best model saved at epoch 12 with Val Loss: 0.0312\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [13/50] Train Loss: 0.0290 Val Loss: 0.0317 PSNR: 22.26 SSIM: 0.6877\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [14/50] Train Loss: 0.0286 Val Loss: 0.0311 PSNR: 22.36 SSIM: 0.6927\n","Best model saved at epoch 14 with Val Loss: 0.0311\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [15/50] Train Loss: 0.0287 Val Loss: 0.0305 PSNR: 22.44 SSIM: 0.6955\n","Best model saved at epoch 15 with Val Loss: 0.0305\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [16/50] Train Loss: 0.0287 Val Loss: 0.0305 PSNR: 22.49 SSIM: 0.6962\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [17/50] Train Loss: 0.0282 Val Loss: 0.0305 PSNR: 22.51 SSIM: 0.6977\n","Best model saved at epoch 17 with Val Loss: 0.0305\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [18/50] Train Loss: 0.0281 Val Loss: 0.0307 PSNR: 22.44 SSIM: 0.6957\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [19/50] Train Loss: 0.0283 Val Loss: 0.0302 PSNR: 22.56 SSIM: 0.6959\n","Best model saved at epoch 19 with Val Loss: 0.0302\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [20/50] Train Loss: 0.0281 Val Loss: 0.0310 PSNR: 22.44 SSIM: 0.6944\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [21/50] Train Loss: 0.0271 Val Loss: 0.0302 PSNR: 22.58 SSIM: 0.6983\n","Best model saved at epoch 21 with Val Loss: 0.0302\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [22/50] Train Loss: 0.0265 Val Loss: 0.0300 PSNR: 22.62 SSIM: 0.6993\n","Best model saved at epoch 22 with Val Loss: 0.0300\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [23/50] Train Loss: 0.0269 Val Loss: 0.0301 PSNR: 22.62 SSIM: 0.6985\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [24/50] Train Loss: 0.0267 Val Loss: 0.0299 PSNR: 22.62 SSIM: 0.6997\n","Best model saved at epoch 24 with Val Loss: 0.0299\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [25/50] Train Loss: 0.0266 Val Loss: 0.0299 PSNR: 22.62 SSIM: 0.6990\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [26/50] Train Loss: 0.0266 Val Loss: 0.0299 PSNR: 22.63 SSIM: 0.7000\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [27/50] Train Loss: 0.0263 Val Loss: 0.0298 PSNR: 22.65 SSIM: 0.7006\n","Best model saved at epoch 27 with Val Loss: 0.0298\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [28/50] Train Loss: 0.0264 Val Loss: 0.0298 PSNR: 22.66 SSIM: 0.7001\n","Best model saved at epoch 28 with Val Loss: 0.0298\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [29/50] Train Loss: 0.0266 Val Loss: 0.0298 PSNR: 22.65 SSIM: 0.7010\n","Best model saved at epoch 29 with Val Loss: 0.0298\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [30/50] Train Loss: 0.0262 Val Loss: 0.0300 PSNR: 22.65 SSIM: 0.6989\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [31/50] Train Loss: 0.0266 Val Loss: 0.0299 PSNR: 22.63 SSIM: 0.6997\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [32/50] Train Loss: 0.0262 Val Loss: 0.0298 PSNR: 22.65 SSIM: 0.7003\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [33/50] Train Loss: 0.0259 Val Loss: 0.0299 PSNR: 22.63 SSIM: 0.6991\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [34/50] Train Loss: 0.0262 Val Loss: 0.0299 PSNR: 22.65 SSIM: 0.7002\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [35/50] Train Loss: 0.0265 Val Loss: 0.0298 PSNR: 22.67 SSIM: 0.7006\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [36/50] Train Loss: 0.0261 Val Loss: 0.0297 PSNR: 22.67 SSIM: 0.7011\n","Best model saved at epoch 36 with Val Loss: 0.0297\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [37/50] Train Loss: 0.0263 Val Loss: 0.0297 PSNR: 22.66 SSIM: 0.7007\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [38/50] Train Loss: 0.0264 Val Loss: 0.0298 PSNR: 22.67 SSIM: 0.7010\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [39/50] Train Loss: 0.0263 Val Loss: 0.0299 PSNR: 22.63 SSIM: 0.7002\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [40/50] Train Loss: 0.0259 Val Loss: 0.0300 PSNR: 22.65 SSIM: 0.6986\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [41/50] Train Loss: 0.0262 Val Loss: 0.0299 PSNR: 22.66 SSIM: 0.6994\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [42/50] Train Loss: 0.0262 Val Loss: 0.0298 PSNR: 22.66 SSIM: 0.7011\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [43/50] Train Loss: 0.0258 Val Loss: 0.0297 PSNR: 22.65 SSIM: 0.7006\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [44/50] Train Loss: 0.0263 Val Loss: 0.0297 PSNR: 22.65 SSIM: 0.7007\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [45/50] Train Loss: 0.0263 Val Loss: 0.0297 PSNR: 22.67 SSIM: 0.7011\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [46/50] Train Loss: 0.0261 Val Loss: 0.0298 PSNR: 22.63 SSIM: 0.6999\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [47/50] Train Loss: 0.0259 Val Loss: 0.0298 PSNR: 22.68 SSIM: 0.7001\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [48/50] Train Loss: 0.0260 Val Loss: 0.0298 PSNR: 22.69 SSIM: 0.7009\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [49/50] Train Loss: 0.0262 Val Loss: 0.0297 PSNR: 22.68 SSIM: 0.7014\n"]},{"output_type":"stream","name":"stderr","text":["                                                                    "]},{"output_type":"stream","name":"stdout","text":["Epoch [50/50] Train Loss: 0.0259 Val Loss: 0.0298 PSNR: 22.68 SSIM: 0.7003\n"]},{"output_type":"stream","name":"stderr","text":["\r"]}],"source":["num_epochs = 50\n","\n","train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs, save_dir)"]},{"cell_type":"markdown","source":["# Step 7: Model Evaluation on Test Dataset"],"metadata":{"id":"875KLIyMHl8V"}},{"cell_type":"markdown","source":["The evaluation of the trained model is performed on the test dataset using the following function. The mean squared error loss is employed as the evaluation criterion. Within a no-gradient context to improve efficiency, the model processes each batch of test inputs, generating outputs that are compared to the ground truth targets to compute the loss. Additionally, quantitative metrics(PSNR and SSIM) are calculated to assess the quality of the reconstructed images. The cumulative loss and metrics are averaged over the entire test set, and the final results are reported to summarize the model’s performance on unseen data."],"metadata":{"id":"dsBLg3RtHgDN"}},{"cell_type":"code","execution_count":14,"metadata":{"id":"YaZxvrJzM7Nu","executionInfo":{"status":"ok","timestamp":1763300703685,"user_tz":0,"elapsed":12,"user":{"displayName":"Reza Ghaedi","userId":"06294103763002776804"}}},"outputs":[],"source":["def test_model(model, test_loader):\n","    total_psnr = 0.0\n","    total_ssim = 0.0\n","    total_loss = 0.0\n","    criterion = torch.nn.MSELoss()  # Loss function for evaluation\n","\n","    with torch.no_grad():\n","        for inputs, targets in test_loader:\n","            inputs = inputs.to(device)\n","            targets = targets.to(device)\n","\n","            outputs = model(inputs)\n","            loss = criterion(outputs, targets)\n","            total_loss += loss.item()\n","\n","            psnr = calculate_psnr(outputs, targets)\n","            ssim = calculate_ssim(outputs, targets)\n","\n","            total_psnr += psnr\n","            total_ssim += ssim\n","\n","    avg_loss = total_loss / len(test_loader)\n","    avg_psnr = total_psnr / len(test_loader)\n","    avg_ssim = total_ssim / len(test_loader)\n","\n","    print(f\"Test Loss: {avg_loss:.4f}\")\n","    print(f\"Average PSNR: {avg_psnr:.2f}\")\n","    print(f\"Average SSIM: {avg_ssim:.4f}\")"]},{"cell_type":"markdown","source":["The path to the previously saved best-performing model is specified and stored in saved_model_path. The available computing device is then determined, selecting a GPU (cuda) if accessible, otherwise defaulting to the CPU. An instance of the WaterNetPlus model is created and transferred to the selected device. The saved model parameters are loaded into this instance, ensuring that the model reflects the state from the best validation epoch. Finally, the model is set to evaluation mode, which disables training-specific behaviors such as dropout and batch normalization updates, preparing it for inference or testing."],"metadata":{"id":"6QJxIANTIGjf"}},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2259,"status":"ok","timestamp":1763301331406,"user":{"displayName":"Reza Ghaedi","userId":"06294103763002776804"},"user_tz":0},"id":"Z5zoVFa3aHmi","outputId":"94ef073c-b47a-4f2f-e73e-d65668c75f97"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","From: https://drive.google.com/uc?id=1DiT9wS7kbYPpbSbxYVqUIM87FALwFOMp\n","To: /content/best_waternet.pth\n","\r  0% 0.00/1.39M [00:00<?, ?B/s]\r 76% 1.05M/1.39M [00:00<00:00, 9.66MB/s]\r100% 1.39M/1.39M [00:00<00:00, 11.8MB/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["WaterNetPlus(\n","  (encoder): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): ResidualBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (se): SEBlock(\n","        (fc): Sequential(\n","          (0): AdaptiveAvgPool2d(output_size=1)\n","          (1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n","          (2): ReLU(inplace=True)\n","          (3): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1))\n","          (4): Sigmoid()\n","        )\n","      )\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (3): ResidualBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (se): SEBlock(\n","        (fc): Sequential(\n","          (0): AdaptiveAvgPool2d(output_size=1)\n","          (1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n","          (2): ReLU(inplace=True)\n","          (3): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1))\n","          (4): Sigmoid()\n","        )\n","      )\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (4): ResidualBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (se): SEBlock(\n","        (fc): Sequential(\n","          (0): AdaptiveAvgPool2d(output_size=1)\n","          (1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n","          (2): ReLU(inplace=True)\n","          (3): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1))\n","          (4): Sigmoid()\n","        )\n","      )\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (middle): Sequential(\n","    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): ResidualBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (se): SEBlock(\n","        (fc): Sequential(\n","          (0): AdaptiveAvgPool2d(output_size=1)\n","          (1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n","          (2): ReLU(inplace=True)\n","          (3): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1))\n","          (4): Sigmoid()\n","        )\n","      )\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (decoder): Sequential(\n","    (0): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): Tanh()\n","  )\n",")"]},"metadata":{},"execution_count":20}],"source":["import os\n","\n","saved_model_path_local = save_dir + \"/\" + \"best_waternet.pth\"\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model = WaterNetPlus().to(device)\n","\n","if os.path.exists(saved_model_path_local):\n","  model.load_state_dict(torch.load(saved_model_path_local, map_location=device))\n","else:\n","  # download from drive:\n","  !gdown --fuzzy \"https://drive.google.com/uc?id=1DiT9wS7kbYPpbSbxYVqUIM87FALwFOMp\"\n","  saved_model_path_local = \"best_waternet.pth\"\n","  model.load_state_dict(torch.load(saved_model_path_local, map_location=device))\n","\n","\n","\n","\n","model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14930,"status":"ok","timestamp":1747142744687,"user":{"displayName":"Reza Ghaedi","userId":"06294103763002776804"},"user_tz":-60},"id":"xzRxqnLUM7O8","outputId":"8eca583a-cf59-426f-fea3-33eb050ff2e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Test Loss: 0.0297\n","Average PSNR: 22.67\n","Average SSIM: 0.7011\n"]}],"source":["test_model(model, val_loader)"]},{"cell_type":"markdown","source":["These metrics provide strong evidence that the WaterNet+ model effectively enhances underwater images. The PSNR score suggests a moderate level of noise reduction, while the SSIM value—being closer to 1—indicates that the model preserves structural and perceptual details well.\n","\n","This objective evaluation complements the visual inspection results and confirms that the model has learned to generalize restoration capabilities across different validation samples.\n","\n"],"metadata":{"id":"Z_ZV0-5uJ0Ve"}},{"cell_type":"markdown","source":["# Step 8: Visual Evaluation of Model Performance on Validation Samples"],"metadata":{"id":"yJa7-d5UIx2Y"}},{"cell_type":"markdown","source":["A random subset of 10 samples is selected from the validation dataset to visually assess the model’s performance. For each chosen sample, the input image and its corresponding target (ground truth) are retrieved. The input image is passed through the trained model in evaluation mode to generate the enhanced output image. All images—the original input, the model’s output, and the target—are then processed for visualization by converting tensor data to NumPy arrays and normalizing the pixel values to a displayable range.\n","\n","These images are displayed side-by-side in a grid layout with three columns representing the input, the model’s output, and the target, respectively. Each row corresponds to one randomly selected sample, allowing for a clear visual comparison across multiple examples. Axis labels are removed to emphasize the image content, and titles are added for clarity."],"metadata":{"id":"o8qV_BgiIrjv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rg1_6HBQbC_G","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1AwpKcVsYVyhbjhTbPijb4h8Kq-8-E3T-"},"executionInfo":{"status":"ok","timestamp":1747142754850,"user_tz":-60,"elapsed":6626,"user":{"displayName":"Reza Ghaedi","userId":"06294103763002776804"}},"outputId":"c72da9a6-c8ab-4d1d-c819-32a8fd298bef"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["import random\n","\n","random_indices = random.sample(range(len(val_dataset)), 10)\n","\n","fig, axes = plt.subplots(nrows=10, ncols=3, figsize=(12, 40))\n","\n","for i, idx in enumerate(random_indices):\n","    image, target = val_dataset[idx]\n","\n","    image_tensor = image.unsqueeze(0).to(device)\n","    target_tensor = target.unsqueeze(0).to(device)\n","\n","    with torch.no_grad():\n","        output = model(image_tensor)\n","\n","    # Output\n","    output_image = output.squeeze().cpu().numpy()\n","    output_image = np.transpose(output_image, (1, 2, 0))\n","    output_image = (output_image + 1) / 2.0\n","\n","    # Target\n","    target_image = target_tensor.squeeze().cpu().numpy()\n","    target_image = np.transpose(target_image, (1, 2, 0))\n","    target_image = (target_image + 1) / 2.0\n","\n","    # Input\n","    input_image = image.cpu().numpy()\n","    input_image = np.transpose(input_image, (1, 2, 0))\n","    input_image = (input_image + 1) / 2.0\n","\n","\n","    axes[i, 0].imshow(input_image)\n","    axes[i, 0].set_title(f\"Input {i+1}\", fontsize=18)\n","    axes[i, 0].axis('off')\n","\n","    axes[i, 1].imshow(output_image)\n","    axes[i, 1].set_title(f\"Output {i+1}\", fontsize=18)\n","    axes[i, 1].axis('off')\n","\n","    axes[i, 2].imshow(target_image)\n","    axes[i, 2].set_title(f\"Target {i+1}\", fontsize=18)\n","    axes[i, 2].axis('off')\n","\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"markdown","source":["# Conclusion"],"metadata":{"id":"yGEHZCueJJQ_"}},{"cell_type":"markdown","source":["This study successfully applied an enhanced deep learning model, WaterNet+, to the task of underwater image restoration. By incorporating residual blocks with Squeeze-and-Excitation (SE) attention mechanisms, the model effectively emphasized important feature channels, leading to improved image reconstruction and richer feature representation. The UIEB dataset, consisting of paired degraded and reference images, was utilized to train and validate the model in a robust and systematic manner.\n","\n","The training process was carefully designed with an adaptive learning rate and a mean squared error loss function, which contributed to steady convergence and optimal performance. Quantitative evaluation through metrics such as Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) confirmed the model’s ability to enhance image quality significantly. Additionally, qualitative visual comparisons illustrated the model’s capability to restore finer image details and reduce underwater distortions.\n","\n","Overall, this work demonstrates the promising potential of combining advanced convolutional architectures with attention mechanisms for challenging image restoration problems, particularly in underwater scenarios where visibility is often compromised."],"metadata":{"id":"LWBgLPMHJIKV"}},{"cell_type":"code","source":[],"metadata":{"id":"d5xWqevzrIwk"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1SRua-8zN9ZLnXOcj7Op1yWEDdknzuBr1","timestamp":1746534269247},{"file_id":"1bnURVwVD6LUzEXbbRf69fQG7k4cMZVqp","timestamp":1746487838992}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}
